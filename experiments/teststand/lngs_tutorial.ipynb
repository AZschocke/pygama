{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# %load_ext snakeviz\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.rcParams['figure.figsize'] = [12, 7] # make a bigger default figure\n",
    "# plt.rcParams['font.size'] = 18\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from pygama import DataSet\n",
    "from pygama.utils import set_plot_style\n",
    "set_plot_style('root')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explore the JSON metadata\n",
    "Pygama currently uses a hand-edited JSON file to define:\n",
    "\n",
    "* Paths to all files (in terms of the env var `$DATADIR`\n",
    "* Processing parameters (chunk size)\n",
    "* Important constants (digitizer clock speed, detector mass, etc.)\n",
    "\n",
    "Hand-edited JSON files are very useful -- you can think of them as a big Python dictionary to store arbitrary parameters.  Calibration constants, run numbers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file\n",
    "db_file = \"testDB.json\"\n",
    "with open(db_file) as f:\n",
    "    testDB = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Top-level information -- \n",
      "loc_dir : $DATADIR/LNGS\n",
      "raw_dir : $DATADIR/LNGS\n",
      "tier_dir : $DATADIR/LNGS/pygama\n",
      "digitizer : ORSIS3302Model\n",
      "t1_prefix : t1_run\n",
      "t2_prefix : t2_run\n",
      "chunksize : 1000\n",
      "clock : 100000000.0\n",
      "rollover : 1\n",
      "mass_note : (5.323 g/cm^3) * ((pi * 3.1^2 * 4.6) cm^3) / 1000\n",
      "det_mass_kg : 0.739\n"
     ]
    }
   ],
   "source": [
    "print(\"-- Top-level information -- \")\n",
    "for key in testDB:\n",
    "    if not isinstance(testDB[key], dict):\n",
    "        print(key, \":\", testDB[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Data set definitions -- \n",
      "{'0': ['204', '2019/03/18 thorium calibration, pos 2 (corner of rm)']}\n"
     ]
    }
   ],
   "source": [
    "print(\"-- Data set definitions -- \")\n",
    "pprint(testDB[\"ds\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. pygama.DataSet\n",
    "\n",
    "pygama uses the `DataSet` object to build a master list of runs and combine them into groups.\n",
    "It uses the JSON DB file to keep track of file paths.\n",
    "The current file uses the `$DATADIR` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw dir :  /home/user/Data/LNGS\n",
      "tier dir :  /home/user/Data/LNGS/pygama\n",
      "t1 file prefix : t1_run\n",
      "t2 file prefix : t2_run\n",
      "current run list : [204]\n",
      "current file paths :\n",
      "{204: {'build_opt': 'conf1',\n",
      "       't0_path': '/home/user/Data/LNGS/2019-3-18-BackgroundRun204',\n",
      "       't1_path': '/home/user/Data/LNGS/pygama/t1_run204.h5',\n",
      "       't2_path': None}}\n"
     ]
    }
   ],
   "source": [
    "# can declare the DataSet either by \"ds\" number, or run numbers.\n",
    "\n",
    "# ds = DataSet(0, 3, md=db_file, v=True) # can use a range of datasets\n",
    "\n",
    "ds = DataSet(run=204, md=db_file, v=True) # can also use a list of run numbers\n",
    "\n",
    "# print some of the DataSet attributes\n",
    "print(\"raw dir : \", ds.raw_dir)\n",
    "print(\"tier dir : \", ds.tier_dir)\n",
    "print(\"t1 file prefix :\", ds.t1pre)\n",
    "print(\"t2 file prefix :\", ds.t2pre)\n",
    "print(\"current run list :\", ds.runs)\n",
    "print(\"current file paths :\")\n",
    "pprint(ds.paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load already processed dataframes\n",
    "\n",
    "# t1df = ds.get_t1df() # currently loads the whole file.  not so useful\n",
    "\n",
    "# t2df = ds.get_t2df() # this is more handy but could be improved upon\n",
    "\n",
    "# timestamps = ds.get_ts() # need to process a run first\n",
    "\n",
    "# runtime = ds.get_runtime() # need to process a run first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tier 0 processing\n",
    "\n",
    "Create a Tier 1 file from an ORCA input file (\"Tier 0\").\n",
    "\n",
    "NOTE: The current Tier numbering may go out of date in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pygama Tier 0 processing ...\n",
      "  Input file: /home/user/Data/LNGS/2019-3-18-BackgroundRun204\n",
      "Total file size: 1505.023 MB\n",
      "Run number: 204\n",
      "Data IDs present in this header are:\n",
      "    4: OR1DHistoDecoder\n",
      "    5: ORRunDecoderForRun\n",
      "    1: ORSIS3302DecoderForEnergy\n",
      "    3: ORSIS3302DecoderForLostData\n",
      "    2: ORSIS3302DecoderForMca\n",
      "pygama will run these decoders:\n",
      "    1: ORSIS3302DecoderForEnergy\n",
      "Overwriting existing file...\n",
      "Beginning Tier 0 processing ...\n",
      "Progress : [####----------------] 20.1% Writing ORSIS3302DecoderForEnergy\n",
      "Progress : [########------------] 40.1% Writing ORSIS3302DecoderForEnergy\n",
      "done.  last packet ID: 100000\n",
      "Writing ORSIS3302DecoderForEnergy\n",
      "Progress : [####################] 100.0% Done...\n",
      "Wrote: Tier 1 File:\n",
      "    /home/user/Data/LNGS/pygama/t1_run204.h5\n",
      "FILE INFO:\n",
      "['/ORSIS3302DecoderForEnergy', '/ORSIS3302Model']\n",
      "File size: 497.202 MB\n",
      "Time elapsed: 46.88 sec\n",
      "Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%prun\n",
    "# %%snakeviz\n",
    "from pygama.io.tier0 import ProcessTier0\n",
    "\n",
    "# settings\n",
    "nevt = 100000\n",
    "overwrite = True\n",
    "test = False\n",
    "verbose = True\n",
    "\n",
    "for run in ds.runs:\n",
    "\n",
    "    t0_file = ds.paths[run][\"t0_path\"]\n",
    "    t1_file = ds.paths[run][\"t1_path\"]\n",
    "    if t1_file is not None and overwrite is False:\n",
    "        continue\n",
    "\n",
    "    conf = ds.paths[run][\"build_opt\"]\n",
    "    opts = ds.runDB[\"build_options\"][conf][\"tier0_options\"]\n",
    "\n",
    "    if test:\n",
    "        print(\"test mode (dry run), processing Tier 0 file:\", t0_file)\n",
    "        continue\n",
    "\n",
    "    if nevt != np.inf:\n",
    "        nevt = int(nevt)\n",
    "\n",
    "    ProcessTier0(\n",
    "        t0_file,\n",
    "        verbose=verbose,\n",
    "        output_dir=ds.tier_dir,\n",
    "        overwrite=overwrite,\n",
    "        n_max=nevt,\n",
    "        settings=opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tier 1 processing (DSP)\n",
    "\n",
    "This can be run in a few different ways:\n",
    "\n",
    "* Batch mode, with multiprocessing.  Right now this is slow at NERSC, but reasonably fast on local machines\n",
    "* Test mode.  By placing `\"test\":1` in the arguments of a Processor, we get a diagnostic (interactive) plot.\n",
    "\n",
    "I prefer to do this by editing the JSON file directly, but for this tutorial, I'll illustrate how the processor list can be treated as a Python dictionary.\n",
    "\n",
    "NOTE: should link the pygama tutorial slides (shows examples of all the Processors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'blsub': {},\n",
      " 'clk': 100000000.0,\n",
      " 'current': [{'sigma': 5, 'wfin': 'wf_blsub', 'wfout': 'wf_current'}],\n",
      " 'dcr': [{'wfin': 'wf_savgol'}, {'wfin': 'wf_pz'}],\n",
      " 'fit_bl': {'ihi': 600, 'order': 1},\n",
      " 'ftp': {},\n",
      " 'get_max': [{'wfin': 'wf_etrap'},\n",
      "             {'wfin': 'wf_strap'},\n",
      "             {'wfin': 'wf_atrap'},\n",
      "             {'wfin': 'wf_ttrap'},\n",
      "             {'wfin': 'wf_savgol'},\n",
      "             {'wfin': 'wf_current'}],\n",
      " 'notch': {'Q': 1, 'f_notch': 30000000.0},\n",
      " 'num_peaks': {},\n",
      " 'overflow': {},\n",
      " 'peakdet': [{'delta': 0.5, 'ihi': 600, 'sigma': 5}],\n",
      " 'pz': {'decay': 82},\n",
      " 'savgol': [{'order': 2,\n",
      "             'wfin': 'wf_blsub',\n",
      "             'wfout': 'wf_savgol',\n",
      "             'window': 47},\n",
      "            {'order': 2,\n",
      "             'wfin': 'wf_notch',\n",
      "             'wfout': 'wf_notsav',\n",
      "             'window': 47}],\n",
      " 'tail_fit': [{'order': 1, 'tp_thresh': 0.8, 'vec': 0, 'wfin': 'wf_savgol'},\n",
      "              {'order': 1, 'tp_thresh': 0.8, 'vec': 0, 'wfin': 'wf_pz'}],\n",
      " 'timepoint': {'pct': [5, 10, 50, 100], 'wfin': 'wf_savgol'},\n",
      " 'trap': [{'decay': 72,\n",
      "           'flat': 2.5,\n",
      "           'rise': 4,\n",
      "           'wfin': 'wf_blsub',\n",
      "           'wfout': 'wf_etrap'},\n",
      "          {'decay': 72,\n",
      "           'flat': 1.5,\n",
      "           'rise': 1,\n",
      "           'wfin': 'wf_notch',\n",
      "           'wfout': 'wf_strap'},\n",
      "          {'fall': 2,\n",
      "           'flat': 0.1,\n",
      "           'rise': 0.04,\n",
      "           'wfin': 'wf_notch',\n",
      "           'wfout': 'wf_atrap'},\n",
      "          {'fall': 1,\n",
      "           'flat': 0,\n",
      "           'rise': 1,\n",
      "           'wfin': 'wf_notch',\n",
      "           'wfout': 'wf_ttrap'}]}\n"
     ]
    }
   ],
   "source": [
    "# show our processor list\n",
    "t1_opts = ds.runDB[\"build_options\"][conf][\"tier1_options\"]\n",
    "\n",
    "# modify one of the processors to have the test argument.  Can also directly edit the JSON file\n",
    "# t1_opts['pz'][\"test\"] = 1\n",
    "\n",
    "# undo the modification\n",
    "# del(t1_opts[\"pz\"][\"test\"]) \n",
    "\n",
    "pprint(t1_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pygama Tier 1 processing ...\n",
      "  Date: 2019-05-28 13:50:06.370345\n",
      "  Input file: /home/user/Data/LNGS/pygama/t1_run204.h5\n",
      "  Size:  497.202 MB\n",
      "Processing data from digitizer: ORSIS3302DecoderForEnergy\n",
      "Found 99991 rows, splitting into 100 chunks\n",
      "Progress : [#-------------------] 6.0% Estimated time to completion: 2.63 min\n",
      "Progress : [#-------------------] 6.0% Estimated time to completion: 2.65 min\n"
     ]
    }
   ],
   "source": [
    "# %%snakeviz\n",
    "# %%prun\n",
    "from pygama.dsp.base import Intercom\n",
    "from pygama.io.tier1 import ProcessTier1\n",
    "\n",
    "# settings\n",
    "nevt = 1000\n",
    "ioff = 0 # starting entry number\n",
    "overwrite = True\n",
    "verbose = True\n",
    "test = False # dry run\n",
    "\n",
    "# set to False when one of the processors is in test mode\n",
    "# or when we want to profile the code with snakeviz\n",
    "multiproc = True\n",
    "\n",
    "# reload the DataSet\n",
    "ds = DataSet(run=204, md=db_file, v=True) # can also use a list of run numbers\n",
    "\n",
    "t1_opts = ds.runDB[\"build_options\"][conf][\"tier1_options\"]\n",
    "# t1_opts['ftp'][\"test\"] = 1\n",
    "\n",
    "# undo the modification\n",
    "# del(t1_opts[\"ftp\"][\"test\"]) \n",
    "\n",
    "\n",
    "for run in ds.runs:\n",
    "\n",
    "    t1_file = ds.paths[run][\"t1_path\"]\n",
    "    t2_file = ds.paths[run][\"t2_path\"]\n",
    "    if t2_file is not None and overwrite is False:\n",
    "        continue\n",
    "        \n",
    "    if test:\n",
    "        print(\"test mode (dry run), processing Tier 1 file:\", t1_file)\n",
    "        continue\n",
    "        \n",
    "    conf = ds.paths[run][\"build_opt\"]\n",
    "    \n",
    "    proc = Intercom(t1_opts)\n",
    "    \n",
    "    # run DSP\n",
    "    ProcessTier1(\n",
    "        t1_file,\n",
    "        proc,\n",
    "        output_dir=ds.tier_dir,\n",
    "        overwrite=overwrite,\n",
    "        verbose=verbose,\n",
    "        multiprocess=multiproc,\n",
    "        nevt=nevt,\n",
    "        ioff=ioff,\n",
    "        chunk=ds.runDB[\"chunksize\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tier 2 analysis\n",
    "\n",
    "Once we have a Tier 2 file, we can do a lot of \"whole-run\" analysis.  \n",
    "\n",
    "Here we'll show:\n",
    "\n",
    "* Timestamp rollover correction\n",
    "* Rough energy calibration\n",
    "* 2615-keV peak resolution\n",
    "\n",
    "For reference, the data columns in the Tier 2 file are:\n",
    "```\n",
    "['channel' 'energy' 'energy_first' 'ievt' 'packet_id' 'timestamp' 'ts_hi'\n",
    " 'bl_rms' 'bl_p0' 'bl_p1' 'etrap_max' 'etrap_imax' 'strap_max'\n",
    " 'strap_imax' 'atrap_max' 'atrap_imax' 'ttrap_max' 'ttrap_imax'\n",
    " 'savgol_max' 'savgol_imax' 'current_max' 'current_imax' 'tp5' 'tp10'\n",
    " 'tp50' 'tp100' 'n_curr_pks' 's_curr_pks' 't0' 't_ftp' 'e_ftp' 'overflow'\n",
    " 'tslope_savgol' 'tslope_pz' 'tail_amp' 'tail_tau']```\n",
    " \n",
    "To compare to \"standard\" analysis parameters:\n",
    "\n",
    "* `e_ftp : trapENF`, recommended energy estimator (Majorana fixed time pickoff algorithm)\n",
    "* `current_max : A` (multisite discrim.)\n",
    "* `atrap_max : T` (slowness)\n",
    "* `tslope_pz : DCR` tail slope measurement, run on pole-zero corrected waveforms\n",
    " \n",
    "Let's start by looking at the timestamps and rollover of the SIS3302 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retrieve the DataFrame from the t2 file.  \n",
    "TODO: add more convenience functions (incl. event limit)\n",
    "\"\"\"\n",
    "# reload the DataSet\n",
    "ds = DataSet(run=204, md=db_file, v=True) # can also use a list of run numbers\n",
    "\n",
    "# get the Tier 2 dataframe\n",
    "t2df = ds.get_t2df() \n",
    "\n",
    "# get the runtime in seconds, accounting for rollover.\n",
    "# NOTE: if the \"nevt\" limit is set, this can give a reduced runtime than the actual run\n",
    "\n",
    "rollover = True\n",
    "\n",
    "timestamps = ds.get_ts(rollover=rollover)\n",
    "plt.plot(timestamps)\n",
    "\n",
    "runtime = ds.get_runtime(rollover=rollover)\n",
    "print(\"runtime: {:.2f} sec\".format(runtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Show waveforms from the Tier 1 file.\n",
    "NOTE: pygama.DataSet has a convenience function \"get_t1df\" but is undeveloped.\n",
    "If there are too many waveforms, we have to use a lot of memory.\n",
    "For now, let's show an example of accessing the file directly with pandas.read_hdf.\n",
    "\"\"\"\n",
    "# t1df = ds.get_t1df() # not ready for use yet\n",
    "\n",
    "# remind ourselves where the file is stored using DataSet\n",
    "# we know it's run 204 already, but use DataSet's members anyway\n",
    "# pprint(ds.paths)\n",
    "run = ds.runs[0]\n",
    "t1_file = ds.paths[run][\"t1_path\"]\n",
    "\n",
    "# remind ourselves the name of the HDF5 group key using the DB.\n",
    "# pprint(testDB['build_options'])\n",
    "t1_key = testDB['build_options']['conf1']['tier0_options']['digitizer']\n",
    "\n",
    "# load a small dataframe\n",
    "t1df = pd.read_hdf(t1_file, stop=200, key=t1_key)\n",
    "t1df.reset_index(inplace=True) # required step -- until we fix pygama 'append' bug\n",
    "\n",
    "print(\"Tier 1 DataFrame columns:\")\n",
    "print(t1df.columns)\n",
    "\n",
    "# scrub the non-wf columns and create a 2d numpy array\n",
    "icols = []\n",
    "for idx, col in enumerate(t1df.columns):\n",
    "    if isinstance(col, int):\n",
    "        icols.append(col)\n",
    "wfs = t1df[icols].values\n",
    "\n",
    "ts = np.arange(0, len(wfs[0]), 1)\n",
    "\n",
    "# one waveform\n",
    "plt.plot(ts, wfs[0])\n",
    "\n",
    "# 50 waveforms\n",
    "for row in wfs[:10]:\n",
    "    plt.plot(ts, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for fun, let's load the T2 dataframe and subtract the baselines manually\n",
    "t2df = ds.get_t2df() \n",
    "\n",
    "# correct a single waveform\n",
    "iwf = 1\n",
    "p0 = t2df[\"bl_p0\"].iloc[1]\n",
    "p1 = t2df[\"bl_p1\"].iloc[1]\n",
    "ts = np.arange(len(wfs[1]))\n",
    "wf = wfs[1] - (p0 + ts * p1)\n",
    "\n",
    "# plt.plot(ts, wf)\n",
    "\n",
    "# correct the whole block at once\n",
    "wfbl = wfs - (p0 + ts * p1)\n",
    "\n",
    "for i in range(50):\n",
    "    plt.plot(ts, wfbl[i])\n",
    "    # plt.plot(ts[:500], wfbl[i][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "show an energy spectrum using e_ftp.\n",
    "method 1: direct use of np.histogram\n",
    "\"\"\"\n",
    "arr = t2df[\"e_ftp\"]\n",
    "\n",
    "xlo, xhi, xpb = 0, 9000, 5\n",
    "nb = int((xhi-xlo)/xpb)\n",
    "h, b = np.histogram(arr, nb, (xlo, xhi))\n",
    "h = np.concatenate((h, [0])) # annoying -- have to add an extra zero\n",
    "\n",
    "plt.semilogy(b, h, ls='steps', lw=1.5, c='b', label=\"pygama e_ftp, {} cts\".format(sum(h)))\n",
    "plt.xlabel(\"Energy (uncal)\", ha='right', x=1)\n",
    "plt.ylabel(\"Counts\", ha='right', y=1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "show an energy spectrum using e_ftp.\n",
    "method 2: try some pygama convenience functions\n",
    "\"\"\"\n",
    "from pygama.analysis.histograms import *\n",
    "\n",
    "arr = t2df[\"e_ftp\"]\n",
    "\n",
    "ph, pb,_ = get_hist(arr, range=(0,9000), dx=5)\n",
    "\n",
    "plot_hist(ph, pb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try out some pygama fitting routines from Jason\n",
    "import pygama.analysis.calibration as pgc\n",
    "import pygama.analysis.histograms as pgh\n",
    "import pygama.utils as pgu\n",
    "\n",
    "# make the hist\n",
    "arr = t2df[\"e_ftp\"]\n",
    "xlo, xhi, xpb = 6000, 7000, 1\n",
    "hist, bins, var = pgh.get_hist(arr, range=(xlo, xhi), dx=xpb)\n",
    "\n",
    "# use a style file and set x title, range\n",
    "pgu.set_plot_style('root')\n",
    "#plt.figure(figsize=(8,6))\n",
    "pgh.plot_hist(hist, bins, var, label=\"data\", show_stats=True, color='black')\n",
    "plt.xlabel(\"energy [ADC]\")\n",
    "plt.gca().set_xlim(6000, 6800)\n",
    "\n",
    "# fit to a gaussian with a standard chi2 fit\n",
    "pars, cov = pgc.fit_hist(pgc.gauss, hist, bins, var=var, guess=[6450, 10, 100])\n",
    "pgu.print_fit_results(pars, cov, pgc.gauss, title=\"chi2 fit\")\n",
    "pgu.plot_func(pgc.gauss, pars, npx=1000, label=\"chi2 fit\", color='cyan')\n",
    "\n",
    "def gauss_plus_step(x, mu, sigma, step, A=1):\n",
    "    return pgc.gauss(x, mu, sigma, A) + np.where(x<mu, step, 0)\n",
    "\n",
    "nbnd = (-np.inf, np.inf)\n",
    "pos = (0, np.inf)\n",
    "pars, cov = pgc.fit_hist(gauss_plus_step, hist, bins, var=var, guess=[6450, 10, 0.1, 100], bounds=[nbnd,pos,pos,pos], poissonLL=True)\n",
    "pgu.print_fit_results(pars, cov, gauss_plus_step, title=\"poissonLL step fit\")\n",
    "pgu.plot_func(gauss_plus_step, pars, npx=1000, label=\"poissonLL step fit\", color='magenta')\n",
    "\n",
    "plt.legend(frameon=False, loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "now let's try calibrating the e_ftp spectrum.\n",
    "NOTE: \n",
    "the code below is from pygama/experiments/mj60/bkg_calibration.py (M. Stortini, UW)\n",
    "but soon the code will be polished and integrated into pygama/pygama/analysis/calibration.py\n",
    "\"\"\"\n",
    "pks_lit = [609.3, 1460.8] \n",
    "\n",
    "from scipy.signal import medfilt\n",
    "\n",
    "hmed = medfilt(h, 5)\n",
    "hpks = h - hmed # subtract out the smoothed spectrum\n",
    "\n",
    "plt.plot(b, h, label=\"e_ftp raw\")\n",
    "plt.plot(b, hmed, label=\"medfilt\")\n",
    "plt.plot(b, hpks, label=\"medfilt-subtracted\") \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run pygama's \"peakdet\" function to automatically identify the peaks\n",
    "\n",
    "maxes, mins = pgu.peakdet(h, 40, b)\n",
    "\n",
    "plt.plot(b, h)\n",
    "# plt.plot(b, hpks)\n",
    "\n",
    "for x,y in maxes:\n",
    "    plt.plot(x, y, \"m.\", ms=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
